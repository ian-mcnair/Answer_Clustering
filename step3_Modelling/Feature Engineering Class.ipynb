{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Math Imports\n",
    "import math\n",
    "\n",
    "# Spacy Imports\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#NLTK Imports\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#string imports\n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "# Transformers and Feature Selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_Features:\n",
    "    \n",
    "    def __init__(self, loadpath, savename, savepath):\n",
    "        self.loadpath = loadpath\n",
    "        self.savename = savename\n",
    "        self.savepath = savepath\n",
    "        self.raw_data = pd.read_csv(\n",
    "            loadpath\n",
    "        )\n",
    "    \n",
    "    def clean_sentence(self, sentence):\n",
    "        sentence = sentence.replace(\".\",\"\").lower().strip()\n",
    "        sentence = sentence.translate(sentence.maketrans(\"\",\"\", string.punctuation))\n",
    "        sentence = sentence.replace(\"  \",\" \")\n",
    "        return sentence\n",
    "\n",
    "    def remove_stopwords(self, doc):\n",
    "        doc = clean_sentence(doc)\n",
    "        my_doc = nlp(doc)\n",
    "        token_list = []\n",
    "        for token in my_doc:\n",
    "            token_list.append(token.text)\n",
    "\n",
    "        filtered_sentence =[] \n",
    "        for word in token_list:\n",
    "            lexeme = nlp.vocab[word]\n",
    "            if lexeme.is_stop == False:\n",
    "                filtered_sentence.append(word) \n",
    "        return \" \".join(filtered_sentence)\n",
    "\n",
    "\n",
    "    def stem_sentence(self, sentence):\n",
    "        # Strip punctuaton\n",
    "        sentence = clean_sentence(sentence)\n",
    "        #Remove Stop Words\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        #Stem or Lem\n",
    "        sentence = clean_sentence(sentence)\n",
    "        return \" \".join([porter.stem(word) for word in sentence.split(\" \")])\n",
    "\n",
    "    def order_sentence(self, sentence):\n",
    "        sentence = clean_sentence(sentence)\n",
    "        sentence = sorted(sentence.split(\" \"))\n",
    "        return \" \".join(sentence)\n",
    "\n",
    "    def word_count(self, sentence):\n",
    "        sentence = sentence.replace(\".\", \"\")\n",
    "        return len(sentence.split(\" \"))\n",
    "\n",
    "#     ################### similarity measures ####################\n",
    "#     def generic_similarity(sentence, answer):\n",
    "#         tokens = set(sentence.split(\" \") + answer.split(\" \"))\n",
    "#         if '' in tokens:\n",
    "#             tokens.remove('')\n",
    "#         sentence_tokens = {token : \"\" for token in tokens}\n",
    "#         count = 0\n",
    "#         for word in tokens:\n",
    "#             if word in sentence.split(\" \"):\n",
    "#                 count += 1\n",
    "#         return count / len(tokens)  \n",
    "\n",
    "#     def jaccard_similarity(student_answer, teacher_answer):\n",
    "#         a = set(student_answer.split(\" \"))\n",
    "#         b = set(teacher_answer.split(\" \"))\n",
    "#         c = a.intersection(b)\n",
    "#         return (len(c) / (len(a) + len(b) - len(c)))\n",
    "\n",
    "\n",
    "#     def cosine_similarity(sentence, answer):\n",
    "#         tokens = set(sentence.split(\" \") + answer.split(\" \"))\n",
    "#         if '' in tokens:\n",
    "#             tokens.remove('')\n",
    "#         answer_tokens = {token : \"\" for token in tokens}\n",
    "#         sentence_tokens = {token : \"\" for token in tokens}\n",
    "\n",
    "#         for word in tokens:\n",
    "#             if word in answer.split(\" \"):\n",
    "#                 answer_tokens[word] = 1\n",
    "#             else:\n",
    "#                 answer_tokens[word] = 0\n",
    "\n",
    "#             if word in sentence.split(\" \"):\n",
    "#                 sentence_tokens[word] = 1\n",
    "#             else:\n",
    "#                 sentence_tokens[word] = 0\n",
    "\n",
    "#         dot_prod = 0\n",
    "#         mag_s = 0\n",
    "#         mag_a = 0\n",
    "#         for word in tokens:\n",
    "#             dot_prod += answer_tokens[word] * sentence_tokens[word]\n",
    "#             mag_s += sentence_tokens[word] ** 2\n",
    "#             mag_a += answer_tokens[word] ** 2\n",
    "\n",
    "#         mag_s = math.sqrt(mag_s)\n",
    "#         mag_a = math.sqrt(mag_a)\n",
    "#         if mag_s * mag_a == 0:\n",
    "#             return 0\n",
    "#         else:\n",
    "#             similarity = dot_prod / (mag_s * mag_a)\n",
    "#             return similarity#round(similarity,4)\n",
    "\n",
    "#     ################### Entity Extraction ####################\n",
    "#     def unigram_entity_extraction(df, sentence_col_name, new_col_name, answer):\n",
    "#         \"\"\"\n",
    "#         This breaks the sentence using spaces\n",
    "#         and then creates features based one each word\n",
    "#         \"\"\"\n",
    "#         answer = answer.replace(\".\",\"\").lower().strip()\n",
    "#         answer = answer.translate(answer.maketrans(\"\",\"\", string.punctuation))\n",
    "#         answer = answer.replace(\"  \",\" \")\n",
    "#         # Break sentence into list\n",
    "#         answer_list = answer.split(\" \")\n",
    "\n",
    "#         for word in answer_list:\n",
    "#             #Goes across each row\n",
    "#             df[f'{new_col_name}_has_{word}'] = df[sentence_col_name].apply(lambda sent: int(word in sent))\n",
    "\n",
    "#         return df\n",
    "\n",
    "#     # Bigram and Trigram still broken\n",
    "#     # Basically, they aren't treating the sentence and answer the same\n",
    "#     def bigram_entity_extraction(df, sentence_col_name, new_col_name, answer):\n",
    "\n",
    "#         answer = clean_sentence(answer)\n",
    "#         # Create list of bigrams for answer\n",
    "#         bigram_answer = create_list_of_bigrams(answer)\n",
    "\n",
    "#         for bigram in bigram_answer:\n",
    "#             bigram_ = bigram.replace(\" \", \"_\")\n",
    "#             df[f'{new_col_name}_has_{bigram_}'] = df[sentence_col_name].apply(lambda sent: int(bigram in sent))\n",
    "\n",
    "#         return df\n",
    "\n",
    "#     def create_list_of_bigrams(sentence):\n",
    "#         sentence = clean_sentence(sentence)\n",
    "#         sentence_list = sentence.split(\" \")\n",
    "#         bigram_list = []\n",
    "\n",
    "#         #For each word in sentence, but needed the index\n",
    "#         for i in range(len(sentence_list)):\n",
    "#             # For index out of bounds error prrevention\n",
    "#             if i < len(sentence_list)-1:\n",
    "#                 bigram_list.append(f\"{sentence_list[i]} {sentence_list[i+1]}\")\n",
    "#     #     print(bigram_list)\n",
    "#         return bigram_list\n",
    "\n",
    "#     def trigram_entity_extraction(df, sentence_col_name, new_col_name, answer):\n",
    "#         answer = clean_sentence(answer)\n",
    "#         trigram_answer = create_list_of_trigrams(answer)\n",
    "\n",
    "#         for trigram in trigram_answer:\n",
    "#             trigram_ = trigram.replace(\" \", \"_\")\n",
    "#             df[f'{new_col_name}_has_{trigram_}'] = df[sentence_col_name].apply(lambda sent: int(trigram in sent))\n",
    "\n",
    "#         return df\n",
    "\n",
    "#     def create_list_of_trigrams(sentence):\n",
    "#         sentence = clean_sentence(sentence)\n",
    "#         sentence_list = sentence.split(\" \")\n",
    "#         trigram_list = []\n",
    "\n",
    "#         #For each word in sentence, but needed the index\n",
    "#         for i in range(len(sentence_list)):\n",
    "#             # For index out of bounds error prrevention\n",
    "#             if i < len(sentence_list)-2:\n",
    "#                 trigram_list.append(f\"{sentence_list[i]} {sentence_list[i+1]} {sentence_list[i+2]}\")\n",
    "\n",
    "#         return trigram_list\n",
    "\n",
    "#     ################### Transforming ####################\n",
    "#     def scale_column(sc, count):\n",
    "#         # Train the scaler on the old data\n",
    "#         # Use the sc to transform the sentence\n",
    "#         return sc.transform(np.array([[count]]))[0][0]\n",
    "\n",
    "#     def save_feature_set(self, df, idx_start, path, filename):\n",
    "#         left = df.iloc[:, :idx_start]\n",
    "#         right = df.iloc[:, idx_start:]\n",
    "\n",
    "#         left.to_csv(\n",
    "#             path + filename + 'doc.csv',\n",
    "#             index = False\n",
    "#         )\n",
    "#         print(\"Saved document data\")\n",
    "#         right.to_csv(\n",
    "#             path + filename + 'data.csv',\n",
    "#             index = False\n",
    "#         )\n",
    "#         print(\"Saved numerical data\")\n",
    "\n",
    "    def create_features(self, data):\n",
    "        a_stopwords = sf.remove_stopwords(self.teacher_answer)\n",
    "        a_stemmed = sf.stem_sentence(a_stopwords)\n",
    "        a_stopwords_ordered = sf.order_sentence(a_stemmed)\n",
    "        a_stemmed_ordered = sf.order_sentence(a_stemmed)\n",
    "        a_lem = sf.lemmatize_sentence(self.teacher_answer)\n",
    "        a_lem_ordered = sf.order_sentence(a_lem)\n",
    "        teacher_answers = [\n",
    "            a_stemmed,\n",
    "            a_stemmed_ordered\n",
    "        ]\n",
    "        \n",
    "        # Change sentence into multiple versions\n",
    "        log = dict()\n",
    "        log['student_answer'] = answer\n",
    "        log['teacher_answer'] = self.teacher_answer\n",
    "        log['q_answer'] = answer\n",
    "        log['q_answer_ordered'] = sf.order_sentence(log['q_answer'])\n",
    "        log['q_stopwords'] = sf.remove_stopwords(answer)\n",
    "        log['q_stopwords_ordered'] = sf.order_sentence(log['q_stopwords'])\n",
    "        log['q_stemmed'] = sf.stem_sentence(answer)\n",
    "        log['q_stem_ordered'] = sf.order_sentence(log['q_stemmed'])\n",
    "        log['q_lemm'] = sf.lemmatize_sentence(answer)\n",
    "        log['q_lemm_ordered'] = sf.order_sentence(log['q_lemm'])\n",
    "        \n",
    "        # Might need to save scaling until jsut before modeling\n",
    "        log['wordcount'] = sf.word_count(answer)\n",
    "        log['wordcount'] = sf.scale_column(self.word_scaler, log['wordcount'])\n",
    "        log['sentence_count'] = sf.sentence_count(answer)\n",
    "        log['sentence_count'] = sf.scale_column(self.sent_scaler, log['sentence_count'])\n",
    "        #same fix as before\n",
    "\n",
    "\n",
    "#         Stem sim\n",
    "        log['stem_g_similarity'] = sf.generic_similarity(log['q_stemmed'], a_stemmed)\n",
    "        log['stem_j_similarity'] = sf.jaccard_similarity(log['q_stemmed'], a_stemmed)\n",
    "        log['stem_c_similarity'] = sf.cosine_similarity(log['q_stemmed'], a_stemmed)\n",
    "        # Ordered\n",
    "        log['stem_ordered_g_similarity'] =  sf.generic_similarity(log['q_stem_ordered'], a_stemmed_ordered)\n",
    "        log['stem_ordered_j_similarity'] =  sf.jaccard_similarity(log['q_stem_ordered'], a_stemmed_ordered)\n",
    "        log['stem_ordered_c_similarity'] =  sf.cosine_similarity(log['q_stem_ordered'], a_stemmed_ordered)\n",
    "\n",
    "  \n",
    "        # Appending New Answer\n",
    "        self.new_answers = self.new_answers.append(log, ignore_index = True)\n",
    "        \n",
    "        # Entity Extraction\n",
    "        types_of_sentences = [\n",
    "            'q_stemmed',\n",
    "            'q_stem_ordered',\n",
    "        ]\n",
    "        \n",
    "        for sent_type, teach_ans in zip(types_of_sentences, teacher_answers):\n",
    "            \n",
    "            self.new_answers = sf.unigram_entity_extraction(self.new_answers, sent_type, sent_type, teach_ans)\n",
    "            self.new_answers = sf.bigram_entity_extraction(self.new_answers, sent_type, sent_type, teach_ans)\n",
    "            self.new_answers = sf.trigram_entity_extraction(self.new_answers, sent_type, sent_type, teach_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
